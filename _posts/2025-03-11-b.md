---
layout: inner
position: center
priority: 1
categories: "texts"

title: "Abstract"
lead_text: "We propose VideoRFSplat, a direct text-to-3D model lever- aging a video generation model to generate realistic 3D Gaussian Splatting (3DGS) for unbounded real-world scenes. 
To generate diverse camera poses and unbounded spatial extent of real-world scenes, while ensuring generalization to arbitrary text prompts, previous methods fine-tune 2D gener-ative models to jointly model camera poses and multi-view images. However, these methods suffer from instability when extending 2D generative models to joint modeling due to the modality gap, which necessitates additional models to stabi-lize training and inference. In this work, we propose an archi-tecture and a sampling strategy to jointly model multi-view images and camera poses when fine-tuning a video genera-tion model. Our core idea is a dual-stream architecture that 014 attaches a dedicated pose generation model alongside a pre- 015 trained video generation model via communication blocks, 016 generating multi-view images and camera poses through 017 separate streams. This design reduces interference between 018 the pose and image modalities. Additionally, we propose an 019 asynchronous sampling strategy that denoises camera poses 020 faster than multi-view images, allowing rapidly denoised 021 poses to condition multi-view generation, reducing mutual 022 ambiguity and enhancing cross-modal consistency. Trained 023 on multiple large-scale real-world datasets (RealEstate10K, 024 MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms 025 existing text-to-3D direct generation methods that heavily de- 026 pend on post-hoc refinement via score distillation sampling, 027 achieving superior results without such refinement."
---
